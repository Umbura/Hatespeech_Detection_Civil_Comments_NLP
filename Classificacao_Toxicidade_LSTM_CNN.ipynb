{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header-setup"
      },
      "source": [
        "# 1. Setup & Configuração"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports-config"
      },
      "outputs": [],
      "source": [
        "# Instalação de dependências\n",
        "!pip install datasets tensorflow scikit-learn pandas imbalanced-learn -q\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from datasets import load_dataset\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from sklearn.utils import class_weight\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Embedding, Bidirectional, LSTM, Dense, Dropout,\n",
        "    Conv1D, GlobalMaxPooling1D, concatenate\n",
        ")\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Hiperparâmetros\n",
        "CONFIG = {\n",
        "    'MAX_WORDS': 20000, 'MAX_LEN': 50, 'EMBEDDING_DIM': 128,\n",
        "    'N_SPLITS': 5, 'BATCH_SIZE': 128, 'EPOCHS': 5,\n",
        "    'TARGET_SAMPLES': 5000, 'TOXICITY_THRESHOLD': 0.5\n",
        "}\n",
        "\n",
        "LABEL_COLUMNS = ['toxicity', 'severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack', 'sexual_explicit']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header-data"
      },
      "source": [
        "# 2. ETL & Balanceamento de Dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "data-processing"
      },
      "outputs": [],
      "source": [
        "def process_data(config, label_cols):\n",
        "    print(\"Carregando dataset...\")\n",
        "    dataset = load_dataset(\"civil_comments\", split='train').to_pandas()\n",
        "\n",
        "    # Definição de rótulos\n",
        "    dataset['non_toxic'] = (dataset[label_cols].sum(axis=1) == 0).astype(int)\n",
        "\n",
        "    def get_label(row):\n",
        "        for label in label_cols:\n",
        "            if row[label] > config['TOXICITY_THRESHOLD']: return label\n",
        "        return 'non_toxic'\n",
        "\n",
        "    dataset['label'] = dataset.apply(get_label, axis=1)\n",
        "    return dataset\n",
        "\n",
        "def balance_data(df, target):\n",
        "    dfs = []\n",
        "    minority_data = []\n",
        "    minority_labels = []\n",
        "\n",
        "    # Undersampling\n",
        "    for label in df['label'].unique():\n",
        "        subset = df[df['label'] == label]\n",
        "        if len(subset) >= target:\n",
        "            dfs.append(subset.sample(n=target, random_state=42))\n",
        "        else:\n",
        "            minority_data.append(subset)\n",
        "            minority_labels.append(label)\n",
        "\n",
        "    # Oversampling\n",
        "    if minority_data:\n",
        "        full_minority = pd.concat(minority_data)\n",
        "        ros = RandomOverSampler(sampling_strategy={l: target for l in minority_labels}, random_state=42)\n",
        "        X_res, y_res = ros.fit_resample(full_minority[['text']], full_minority['label'])\n",
        "        dfs.append(pd.DataFrame({'text': X_res['text'], 'label': y_res}))\n",
        "\n",
        "    return pd.concat(dfs).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "df_raw = process_data(CONFIG, LABEL_COLUMNS)\n",
        "df_final = balance_data(df_raw, CONFIG['TARGET_SAMPLES'])\n",
        "\n",
        "print(f\"Dataset final: {len(df_final)} amostras.\")\n",
        "print(df_final['label'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header-tokenization"
      },
      "source": [
        "# 3. Tokenização"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tokenization"
      },
      "outputs": [],
      "source": [
        "le = LabelEncoder()\n",
        "y_encoded = to_categorical(le.fit_transform(df_final['label']))\n",
        "\n",
        "tokenizer = Tokenizer(num_words=CONFIG['MAX_WORDS'])\n",
        "tokenizer.fit_on_texts(df_final['text'])\n",
        "X_padded = pad_sequences(tokenizer.texts_to_sequences(df_final['text']), maxlen=CONFIG['MAX_LEN'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header-model"
      },
      "source": [
        "# 4. Arquitetura do Modelo (Híbrido)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "model-arch"
      },
      "outputs": [],
      "source": [
        "def build_model(cfg, n_classes):\n",
        "    inp = Input(shape=(cfg['MAX_LEN'],))\n",
        "    emb = Embedding(cfg['MAX_WORDS'], cfg['EMBEDDING_DIM'])(inp)\n",
        "\n",
        "    # LSTM Branch\n",
        "    lstm = Bidirectional(LSTM(32, return_sequences=False))(emb)\n",
        "\n",
        "    # CNN Branch\n",
        "    cnns = [GlobalMaxPooling1D()(Conv1D(32, k, activation='relu')(emb)) for k in [2, 3, 4]]\n",
        "    cnn = concatenate(cnns)\n",
        "\n",
        "    # Merge\n",
        "    x = concatenate([lstm, cnn])\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    out = Dense(n_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inp, out)\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "build_model(CONFIG, len(le.classes_)).summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header-train"
      },
      "source": [
        "# 5. Treinamento (Stratified K-Fold)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "training"
      },
      "outputs": [],
      "source": [
        "skf = StratifiedKFold(n_splits=CONFIG['N_SPLITS'], shuffle=True, random_state=42)\n",
        "class_weights = dict(enumerate(class_weight.compute_class_weight(\n",
        "    'balanced', classes=np.unique(le.transform(df_final['label'])), y=le.transform(df_final['label']))))\n",
        "\n",
        "histories, f1_scores, acc_scores = [], [], []\n",
        "\n",
        "# Listas para armazenar todos os rótulos verdadeiros e preditos de todas as folds\n",
        "all_y_true = []\n",
        "all_y_pred = []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(X_padded, np.argmax(y_encoded, axis=1)), 1):\n",
        "    print(f\"\\nFold {fold}/{CONFIG['N_SPLITS']}\")\n",
        "\n",
        "    model = build_model(CONFIG, len(le.classes_))\n",
        "    history = model.fit(\n",
        "        X_padded[train_idx], y_encoded[train_idx],\n",
        "        validation_data=(X_padded[val_idx], y_encoded[val_idx]),\n",
        "        epochs=CONFIG['EPOCHS'], batch_size=CONFIG['BATCH_SIZE'],\n",
        "        callbacks=[EarlyStopping(patience=2, restore_best_weights=True)],\n",
        "        class_weight=class_weights, verbose=1\n",
        "    )\n",
        "\n",
        "    histories.append(history)\n",
        "    y_pred = np.argmax(model.predict(X_padded[val_idx]), axis=1)\n",
        "    y_true = np.argmax(y_encoded[val_idx], axis=1)\n",
        "\n",
        "    # Armazenar rótulos verdadeiros e preditos para avaliação geral\n",
        "    all_y_true.extend(y_true)\n",
        "    all_y_pred.extend(y_pred)\n",
        "\n",
        "    f1_scores.append(f1_score(y_true, y_pred, average='macro'))\n",
        "    acc_scores.append(np.mean(y_pred == y_true))\n",
        "    print(f\"Fold {fold} F1: {f1_scores[-1]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header-eval"
      },
      "source": [
        "# 6. Avaliação Final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evaluation"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, f1_score, recall_score, accuracy_score\n",
        "\n",
        "def plot_cv_history(histories, f1_scores):\n",
        "    max_epochs = max([len(h.history['val_loss']) for h in histories])\n",
        "\n",
        "    val_accs = []\n",
        "    val_losses = []\n",
        "    for h in histories:\n",
        "        acc = h.history['val_accuracy']\n",
        "        loss = h.history['val_loss']\n",
        "        acc_padded = np.pad(acc, (0, max_epochs - len(acc)), 'edge')\n",
        "        loss_padded = np.pad(loss, (0, max_epochs - len(loss)), 'edge')\n",
        "        val_accs.append(acc_padded)\n",
        "        val_losses.append(loss_padded)\n",
        "\n",
        "    val_accs = np.array(val_accs)\n",
        "    val_losses = np.array(val_losses)\n",
        "\n",
        "    acc_mean = np.mean(val_accs, axis=0)\n",
        "    acc_std = np.std(val_accs, axis=0)\n",
        "    loss_mean = np.mean(val_losses, axis=0)\n",
        "    loss_std = np.std(val_losses, axis=0)\n",
        "    epochs = range(1, max_epochs + 1)\n",
        "\n",
        "    plt.figure(figsize=(18, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, acc_mean, 'b-', label='Acurácia de Validação (Média)')\n",
        "    plt.fill_between(epochs, acc_mean - acc_std, acc_mean + acc_std, color='blue', alpha=0.2, label='Desvio Padrão (Acurácia)')\n",
        "    plt.plot(epochs, loss_mean, 'r-', label='Perda de Validação (Média)')\n",
        "    plt.fill_between(epochs, loss_mean - loss_std, loss_mean + loss_std, color='red', alpha=0.2, label='Desvio Padrão (Perda)')\n",
        "    plt.title('Desempenho Médio do Modelo nos Folds')\n",
        "    plt.xlabel('Épocas')\n",
        "    plt.ylabel('Acurácia / Perda')\n",
        "    plt.legend(loc='best')\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    fold_labels = [f'Fold {i+1}' for i in range(len(f1_scores))]\n",
        "    f1_mean = np.mean(f1_scores)\n",
        "    bars = plt.bar(fold_labels, f1_scores, color='skyblue', label='F1-Score por Fold')\n",
        "    plt.axhline(f1_mean, color='crimson', linestyle='--', linewidth=2, label=f'F1 Médio: {f1_mean:.4f}')\n",
        "    for bar in bars:\n",
        "        yval = bar.get_height()\n",
        "        plt.text(bar.get_x() + bar.get_width()/2.0, yval + 0.005, f'{yval:.4f}', ha='center', va='bottom')\n",
        "    plt.title('F1-Score (Macro) por Fold de Validação')\n",
        "    plt.xlabel('Fold')\n",
        "    plt.ylabel('F1-Score')\n",
        "    plt.ylim(top=max(f1_scores) * 1.1)\n",
        "    plt.legend(loc='best')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\n--- Avaliação Geral da Cross-Validation ---\")\n",
        "all_y_true = np.array(all_y_true)\n",
        "all_y_pred = np.array(all_y_pred)\n",
        "\n",
        "overall_f1_macro = f1_score(all_y_true, all_y_pred, average='macro')\n",
        "overall_recall_macro = recall_score(all_y_true, all_y_pred, average='macro')\n",
        "overall_accuracy = accuracy_score(all_y_true, all_y_pred)\n",
        "\n",
        "print(f\"Acurácia Geral: {overall_accuracy:.4f}\")\n",
        "print(f\"F1-Score Geral (Macro): {overall_f1_macro:.4f}\")\n",
        "print(f\"Recall Geral (Macro): {overall_recall_macro:.4f}\")\n",
        "\n",
        "print(\"\\nRelatório de Classificação Detalhado:\")\n",
        "print(classification_report(all_y_true, all_y_pred, target_names=le.classes_))\n",
        "\n",
        "plot_cv_history(histories, f1_scores)"
      ]
    }
  ]
}
